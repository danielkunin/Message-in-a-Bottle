Authors: MSB
Date: Dec 14th,2017

# Moon Base

This is a readme for our code built off of https://github.com/ravidziv/IDNNs


Tasks of Interest:

To train a model with 5 layer NN:
run mainMSB.py
This file will:
+take the architecture defined in network_parameters.py
+train it over the described number of epochs with fixed minibatch size
+repeat 20 times and save to file each time to study the mean effect in the information plane
+the saved .npy files are used in AverageInfoPlane.py and CompressionWithDepth.py

To plot the learning curve as a function of epochs and the corrsponding information plane for an underfit, overfit and just right fit:
run AverageInfoPlane.py
This simply reads the files you wrote out in runMSB.py in the mode of generating many replicates

To plot up the compression curves versus generalization gap:
run compressionwithdepthmansheej.py
This again reads files generated by runMSB.py to plot up for various network architectures the learning dynamics of the last layer in the information plane.

To plot up moondata and run batch gradient descent on it with a single hidden neuron layer:
run MSB_moondata.py
This will give you the decision boundary plot in a form borrowed from Denny Britz
These are nice for illustraing the fits in clear way on SciKit-Learns moondata.

To run a convolutional NN on MNIST with partial implementation of the information plane (incomplete):
run MSB_CovNET_TF.py
This plots up the learning curve and easily achieves sub-1% error on MNIST
The information plane is in progress but has not been completed for submission of the final project

To run a simple softmax classifier which achieves 92% accuracy on MNIST:
run MSB_softmaxTF.py
This file is a development file but gives important intution for the difficulty of MNIST classification (i.e. its not the hardest thing to do reasonbly well on).

To learn more about MNIST, we looked at Hu's Moment Invarients:
run HusMoments.py
you will find the application of Hu's Moment Invarients (which are measures that are invarient to scale, translation and rotation of the feature in the image). We use this to reduce the dimensionality of MNIST down from 784 to 8. We then study the classifications as a function of their position in this low dimensional space and plot up results projecting this data down to two principle components.



How we built off this existing codebase from:
1. Replace MNIST with Moondata (comprehesive changes to the file utils.loaddata())
- importing scikit-learns moon data set
- converting labels to one-hots
- adding N features of Gaussian noise
- porting out

2. Overhaul of the InfoNetworks file
- Introduction of dev accuracy calculation and train accuracy calculation
- porting these values out to our learning curve from our TF session
- adding the 

3. modifications to fix error in mutual_info_estimation.estimate_IY_by_network
- data handling produced errors, had to improve this pipeline.

4. Adding the path to the calculation of the variational information.
Varitional information is included but not used in the code base.

5. Wrote new plotting output for everything we displayed.
-Learning curves
-Information Plane
-Four quadrants of log-error plane to information plane
-Decision Boundaries
-added the log-timespacing for calculational tractability (reduces run time to something managable)

6. Modified the network otimizer from Adam to SGD
- This is important because all of the published results state the use of SGD and not a modified SDG

7. Improved the architecture selection in network_parameters.py
- Formerly only called preset defaults with special calculations associated with each
- Now it allows you to select arbitrary network shape with a simple change in the bottom of the file and a quick change to your MainMSB.py extractlastlayer function.

8. Extensive Modification of network.py (where the network is trained and the TF session is openned)
- fixed an error in extracting activities, was inputing incorrect output dimensions.
- 

9. Implemetned the Convolutionarl NN with <1% error using TF (first time using Tensorflow)
- Simple architecture, run on GPU (970 GTX) with run time of about 10Min.

10.Used the computer vision package openCV to implement the Hu's moments.
- Calculated the moment structure within our own function which feeds into the OpenCV Hu's moments function

